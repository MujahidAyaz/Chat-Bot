{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DialoGPT Chatbot in Google Colab\n",
        "\n",
        "This notebook demonstrates how to create an interactive chatbot using the pre-trained **DialoGPT** model by Microsoft. It uses the Hugging Face `transformers` library to generate conversational responses.\n",
        "\n",
        "## Step 1: Install Necessary Libraries\n",
        "\n",
        "First, we need to install the Hugging Face `transformers` library.\n",
        "\n",
        "```python\n",
        "!pip install transformers --quiet\n"
      ],
      "metadata": {
        "id": "wZ4U62UrZcol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Load the Pre-trained Model and Tokenizer\n",
        "Next, we load the DialoGPT-medium model and tokenizer, which will allow us to generate text responses based on user input."
      ],
      "metadata": {
        "id": "gvq4JpG9Z3Uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load a pre-trained model and tokenizer (DialoGPT in this case)\n",
        "model_name = \"microsoft/DialoGPT-medium\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "pEdleyPBZj3E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Define the Chatbot Function\n",
        "Now, we define a function that will take user input, process it, and generate a response."
      ],
      "metadata": {
        "id": "6SvdFUNrZ7Vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chatbot function\n",
        "def chat_with_model(user_input, chat_history_ids=None):\n",
        "    # Tokenize user input\n",
        "    input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors=\"pt\")\n",
        "\n",
        "    # Generate response\n",
        "    bot_output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=1000,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,  # Adjust for creativity\n",
        "    )\n",
        "\n",
        "    # Decode the response\n",
        "    bot_response = tokenizer.decode(bot_output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "    return bot_response\n"
      ],
      "metadata": {
        "id": "c9NZDqQdZwVD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Start the Chatbot\n",
        "The following code starts an interactive chat. The bot will keep responding until you type \"exit\", \"quit\", or \"bye\"."
      ],
      "metadata": {
        "id": "fKX9YeOTaBzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the chatbot\n",
        "print(\"Chatbot is ready! Type 'exit' to quit.\")\n",
        "chat_history = None\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    response = chat_with_model(user_input, chat_history)\n",
        "    print(f\"Chatbot: {response}\")\n"
      ],
      "metadata": {
        "id": "Gfgq5MrFaGZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to Use\n",
        "Run the entire notebook.\n",
        "The chatbot will greet you and wait for your input.\n",
        "Type your messages in the input field.\n",
        "To exit the conversation, type exit, quit, or bye.\n"
      ],
      "metadata": {
        "id": "j-RA0NabaPxK"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}